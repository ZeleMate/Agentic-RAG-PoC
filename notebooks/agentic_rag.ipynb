{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bef23e9",
   "metadata": {},
   "source": [
    "## Agentic RAG â€“ Proof of Concept (LangGraph + FAISS + Hugging Face + Ollama)\n",
    "\n",
    "# Goal: arXiv paper -> chunks -> embeddings -> FAISS -> retrieve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afa1856",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "This notebook is a minimal, local Agentic RAG proof-of-concept. The following cell installs only the packages required to:\n",
    "- load and chunk documents\n",
    "- embed chunks and build a FAISS vector index\n",
    "- run a local LLM via `init_chat_model` (e.g., Ollama)\n",
    "- orchestrate agentic control flow with LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11206043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (4.55.2)\n",
      "Requirement already satisfied: langgraph in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (0.6.5)\n",
      "Requirement already satisfied: langchain-community in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-text-splitters in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (0.3.9)\n",
      "Requirement already satisfied: langchain-ollama in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (0.3.6)\n",
      "Requirement already satisfied: langchain-huggingface in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (0.3.1)\n",
      "Requirement already satisfied: sentence-transformers in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (5.1.0)\n",
      "Requirement already satisfied: torch in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: langchain-docling in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: faiss-cpu in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (1.12.0)\n",
      "Requirement already satisfied: filelock in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
      "Requirement already satisfied: langchain-core>=0.1 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langgraph) (0.3.74)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langgraph) (2.1.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langgraph) (0.6.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langgraph) (0.2.0)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langgraph) (2.11.7)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph) (3.11.2)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langchain-community) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langchain-community) (2.0.43)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langchain-community) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langchain-community) (0.4.14)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.5.1 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langchain-ollama) (0.5.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: scipy in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from sentence-transformers) (1.16.1)\n",
      "Requirement already satisfied: Pillow in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: docling~=2.18 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langchain-docling) (2.44.0)\n",
      "Requirement already satisfied: docling-core<3.0.0,>=2.42.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling-core[chunking]<3.0.0,>=2.42.0->docling~=2.18->langchain-docling) (2.44.2)\n",
      "Requirement already satisfied: docling-parse<5.0.0,>=4.0.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling~=2.18->langchain-docling) (4.1.0)\n",
      "Requirement already satisfied: docling-ibm-models<4,>=3.9.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling~=2.18->langchain-docling) (3.9.0)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling~=2.18->langchain-docling) (1.2.0)\n",
      "Requirement already satisfied: pypdfium2!=4.30.1,<5.0.0,>=4.30.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling~=2.18->langchain-docling) (4.30.0)\n",
      "Requirement already satisfied: easyocr<2.0,>=1.7 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling~=2.18->langchain-docling) (1.7.2)\n",
      "Requirement already satisfied: rtree<2.0.0,>=1.3.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling~=2.18->langchain-docling) (1.4.1)\n",
      "Requirement already satisfied: typer<0.17.0,>=0.12.5 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling~=2.18->langchain-docling) (0.16.0)\n",
      "Requirement already satisfied: python-docx<2.0.0,>=1.1.2 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling~=2.18->langchain-docling) (1.2.0)\n",
      "Requirement already satisfied: python-pptx<2.0.0,>=1.0.2 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling~=2.18->langchain-docling) (1.0.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling~=2.18->langchain-docling) (4.13.4)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling~=2.18->langchain-docling) (2.3.1)\n",
      "Requirement already satisfied: marko<3.0.0,>=2.1.2 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling~=2.18->langchain-docling) (2.2.0)\n",
      "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling~=2.18->langchain-docling) (3.1.5)\n",
      "Requirement already satisfied: lxml<6.0.0,>=4.0.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling~=2.18->langchain-docling) (5.4.0)\n",
      "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling~=2.18->langchain-docling) (1.6.0)\n",
      "Requirement already satisfied: pylatexenc<3.0,>=2.10 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling~=2.18->langchain-docling) (2.10)\n",
      "Requirement already satisfied: accelerate<2,>=1.0.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling~=2.18->langchain-docling) (1.10.0)\n",
      "Requirement already satisfied: psutil in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from accelerate<2,>=1.0.0->docling~=2.18->langchain-docling) (7.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling~=2.18->langchain-docling) (2.7)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling~=2.18->langchain-docling) (4.25.0)\n",
      "Requirement already satisfied: jsonref<2.0.0,>=1.1.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling~=2.18->langchain-docling) (1.1.0)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling~=2.18->langchain-docling) (0.9.0)\n",
      "Requirement already satisfied: latex2mathml<4.0.0,>=3.77.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling~=2.18->langchain-docling) (3.78.0)\n",
      "Requirement already satisfied: semchunk<3.0.0,>=2.2.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling-core[chunking]<3.0.0,>=2.42.0->docling~=2.18->langchain-docling) (2.2.2)\n",
      "Requirement already satisfied: torchvision<1,>=0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling-ibm-models<4,>=3.9.0->docling~=2.18->langchain-docling) (0.23.0)\n",
      "Requirement already satisfied: jsonlines<4.0.0,>=3.1.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling-ibm-models<4,>=3.9.0->docling~=2.18->langchain-docling) (3.1.0)\n",
      "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.6.0.66 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from docling-ibm-models<4,>=3.9.0->docling~=2.18->langchain-docling) (4.12.0.88)\n",
      "Requirement already satisfied: scikit-image in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from easyocr<2.0,>=1.7->docling~=2.18->langchain-docling) (0.25.2)\n",
      "Requirement already satisfied: python-bidi in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from easyocr<2.0,>=1.7->docling~=2.18->langchain-docling) (0.6.6)\n",
      "Requirement already satisfied: Shapely in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from easyocr<2.0,>=1.7->docling~=2.18->langchain-docling) (2.1.1)\n",
      "Requirement already satisfied: pyclipper in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from easyocr<2.0,>=1.7->docling~=2.18->langchain-docling) (1.3.0.post6)\n",
      "Requirement already satisfied: ninja in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from easyocr<2.0,>=1.7->docling~=2.18->langchain-docling) (1.13.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling~=2.18->langchain-docling) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling~=2.18->langchain-docling) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling~=2.18->langchain-docling) (0.27.0)\n",
      "Requirement already satisfied: et-xmlfile in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from openpyxl<4.0.0,>=3.1.5->docling~=2.18->langchain-docling) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from pandas<3.0.0,>=2.1.4->docling~=2.18->langchain-docling) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from pandas<3.0.0,>=2.1.4->docling~=2.18->langchain-docling) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from pandas<3.0.0,>=2.1.4->docling~=2.18->langchain-docling) (2025.2)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from python-pptx<2.0.0,>=1.0.2->docling~=2.18->langchain-docling) (3.2.5)\n",
      "Requirement already satisfied: mpire[dill] in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.42.0->docling~=2.18->langchain-docling) (2.10.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from typer<0.17.0,>=0.12.5->docling~=2.18->langchain-docling) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from typer<0.17.0,>=0.12.5->docling~=2.18->langchain-docling) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from typer<0.17.0,>=0.12.5->docling~=2.18->langchain-docling) (14.1.0)\n",
      "Requirement already satisfied: anyio in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (0.16.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.4->docling~=2.18->langchain-docling) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<0.17.0,>=0.12.5->docling~=2.18->langchain-docling) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<0.17.0,>=0.12.5->docling~=2.18->langchain-docling) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.17.0,>=0.12.5->docling~=2.18->langchain-docling) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.15 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.42.0->docling~=2.18->langchain-docling) (0.70.16)\n",
      "Requirement already satisfied: dill>=0.3.8 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.42.0->docling~=2.18->langchain-docling) (0.3.8)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from scikit-image->easyocr<2.0,>=1.7->docling~=2.18->langchain-docling) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from scikit-image->easyocr<2.0,>=1.7->docling~=2.18->langchain-docling) (2025.6.11)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from scikit-image->easyocr<2.0,>=1.7->docling~=2.18->langchain-docling) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Only run this cell if you are not using the environment with the dependencies installed from the pyproject.toml file\n",
    "\n",
    "%pip install -U \\\n",
    "  transformers \\\n",
    "  langgraph langchain-community langchain-text-splitters langchain-ollama langchain-huggingface\\\n",
    "  sentence-transformers \\\n",
    "  torch langchain-docling \\\n",
    "  faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b49efb6",
   "metadata": {},
   "source": [
    "## Imports and reproducibility\n",
    "We import only what is needed for a clear, reproducible baseline. A fixed random seed makes the pipeline deterministic for demonstration.\n",
    "\n",
    "Key points:\n",
    "- keep imports minimal (clarity over breadth)\n",
    "- set a global seed for `random` and `numpy`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05ef7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045d916",
   "metadata": {},
   "source": [
    "## Document loading and chunking\n",
    "We use Docling to load a single arXiv PDF and split it into chunks suitable for embedding. Two export modes are supported:\n",
    "- `DOC_CHUNKS` (default): uses the loader's hybrid chunker\n",
    "- `MARKDOWN`: uses header-based splitting (shown for completeness)\n",
    "\n",
    "This step turns raw text into semantically meaningful pieces for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d681ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/zelenyianszkimate/Documents/Agentic-RAG-PoC/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5188 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- d.page_content='A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures\\nDezhang Kong, Shi Lin, Zhenhua Xu, Zhebo Wang, Minghao Li, Yufeng Li, Yilun Zhang, Hujin Peng, Zeyang Sha, Yuyuan Li, Changting Lin, Xun Wang, Xuan Liu, Ningyu Zhang, Chaochao Chen, Muhammad Khurram Khan, Meng Han'\n",
      "- d.page_content='A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures\\nAbstract -In recent years, Large-Language-Model-driven AI agents have exhibited unprecedented intelligence and adaptability, and are rapidly changing human production and life. Nowadays, agents are undergoing a new round of evolution. They no longer act as an isolated island like LLMs. Instead, they start to communicate with diverse external entities, such as other agents and tools, to perform more complex tasks collectively. Under this trend, agent communication is regarded as a foundational pillar of the future AI ecosystem, and many organizations have intensively begun'\n",
      "- d.page_content=\"A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures\\nto design related communication protocols (e.g., Anthropic's MCP and Google's A2A) within the recent few months. However, this new field exposes significant security hazards, which can cause severe damage to real-world scenarios. To help researchers quickly figure out this promising topic and benefit the future agent communication development, this paper presents a comprehensive survey of agent communication security . More precisely, we first present a clear definition of agent communication and categorize the entire lifecycle of agent communication into\"\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "from langchain_docling import DoclingLoader\n",
    "from langchain_docling.loader import ExportType\n",
    "\n",
    "FILE_PATH = [\"https://arxiv.org/pdf/2506.19676\"]\n",
    "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "GEN_MODEL_ID = \"ollama:qwen3:4b\"\n",
    "EXPORT_TYPE = ExportType.DOC_CHUNKS\n",
    "\n",
    "loader = DoclingLoader(\n",
    "    file_path=FILE_PATH,\n",
    "    export_type=EXPORT_TYPE,\n",
    "    chunker=HybridChunker(tokenizer=EMBED_MODEL_ID),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "if EXPORT_TYPE == ExportType.DOC_CHUNKS:\n",
    "    splits = docs\n",
    "elif EXPORT_TYPE == ExportType.MARKDOWN:\n",
    "    from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "    splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[\n",
    "            (\"#\", \"Header_1\"),\n",
    "            (\"##\", \"Header_2\"),\n",
    "            (\"###\", \"Header_3\"),\n",
    "        ],\n",
    "    )\n",
    "    splits = [split for doc in docs for split in splitter.split_text(doc.page_content)]\n",
    "else:\n",
    "    raise ValueError(f\"Unexpected export type: {EXPORT_TYPE}\")\n",
    "\n",
    "for d in splits[:3]:\n",
    "    print(f\"- {d.page_content=}\")\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e13af6d",
   "metadata": {},
   "source": [
    "## Vector store and retriever\n",
    "We embed the chunks with `sentence-transformers` and build a FAISS index. Then we expose a `retriever` and wrap it as a LangChain \"tool\" so the agent can decide to call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5e99f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FAISS index from documents...\n",
      "FAISS index size: 695\n",
      "Number of document chunks: 695\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL_ID)\n",
    "\n",
    "print(\"Building FAISS index from documents...\")\n",
    "vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"arxiv_paper_retriever\",\n",
    "    \"Retrieves and answers the question about the arxiv paper\",\n",
    ")\n",
    "\n",
    "print(f\"FAISS index size: {vectorstore.index.ntotal}\")\n",
    "print(f\"Number of document chunks: {len(splits)}\")\n",
    "\n",
    "# If the index is empty, add documents:\n",
    "if vectorstore.index.ntotal == 0:\n",
    "    print(\"Populating the FAISS index...\")\n",
    "    vectorstore.add_documents(splits)\n",
    "    print(f\"FAISS index size after update: {vectorstore.index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10685490",
   "metadata": {},
   "source": [
    "## Agentic decision: retrieve or respond\n",
    "We initialize a local chat model and define a function that lets the LLM decide autonomously whether to call the retriever tool or answer directly.\n",
    "\n",
    "This demonstrates agentic behavior: tool-use planning vs direct generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9f4164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "response_model = init_chat_model(GEN_MODEL_ID, temperature=0)\n",
    "\n",
    "\n",
    "def generate_query_or_respond(state: MessagesState):\n",
    "    \"\"\"\n",
    "    Call the model to generate a response based on the current state.\n",
    "    The LLM will decide to use the retriever tool, or respond directly.\n",
    "    \"\"\"\n",
    "    response = response_model.bind_tools([retriever_tool]).invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68c25308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, the user is asking about the main idea of an arXiv paper. But wait, they didn't specify which paper. Hmm, the function provided is arxiv_paper_retriever, which requires a query. Since the user didn't mention a specific paper title or ID, I can't retrieve the main idea without knowing which paper they're referring to.\n",
      "\n",
      "I should check the tools again. The function needs a \"query\" parameter. The user's question is too vague here. Maybe they expect me to ask for more details, but according to the instructions, I need to call the function if possible. Wait, the problem says \"you may call one or more functions to assist with the user query.\" But in this case, the user hasn't provided enough info. So perhaps I should respond that I need more details, but the instructions say to return tool calls within the XML tags.\n",
      "\n",
      "Wait, the user's message is \"What is the main idea of the arxiv paper?\" without specifying which paper. The function requires a query. So I can't call the function without a specific query. Therefore, I should inform the user that they need to specify the paper title or ID. But the problem says to return tool calls if possible. Since there's no query provided, maybe I shouldn't make a tool call here. Wait, the instructions say \"For each function call, return a json object...\" but if there's no valid query, perhaps I shouldn't call the function. However, the user's question is too vague, so the correct response is to ask for more information. But the problem states that I should use the tools if possible. Hmm.\n",
      "\n",
      "Wait, the problem says \"You may call one or more functions to assist with the user query.\" So if the user's query is incomplete, I shouldn't call the function. But in this case, the user didn't specify which paper, so the function can't be called. Therefore, I should not generate any tool calls here. But the instructions say to return tool calls within the XML tags if I call them. Since I can't, maybe I should respond with a message. Wait, but the problem says \"For each function call, return a json object...\" So if there's no function call needed, I shouldn't output any tool_call tags. But the user's question is a bit unclear.\n",
      "\n",
      "Wait, the user might have intended to ask about a specific paper but forgot to mention it. Since the function requires a query, I need to prompt them for the paper title or ID. However, the problem here is that the assistant is supposed to call the function if possible. Since the user didn't provide a query, the correct action is to not call the function and instead ask for more details. But according to the problem's instructions, the assistant should only output tool calls if they need to. In this case, since the query is missing, there's no valid tool call. So the answer would be to not generate any tool_call. But the problem says \"You may call one or more functions...\" so if it's not possible, then no tool calls.\n",
      "\n",
      "Wait, but the user's question is \"What is the main idea of the arxiv paper?\" which is a bit ambiguous. Maybe they mean any arXiv paper, but that's not possible. The function requires a specific query. So the correct response here is to inform the user that they need to specify which paper they're asking about. However, the problem's instructions say that the assistant should return tool calls within the XML tags if they need to. Since the user didn't provide enough info, there's no tool call to make. Therefore, I shouldn't output any tool_call tags here. But the problem might expect me to generate a tool call with a placeholder query? Wait, no. The function requires a \"query\" parameter, which is a string. Since the user didn't specify, I can't fill that in. So the answer is that no tool call is made here. But the problem says \"For each function call...\" so if there's none, then no tool_call.\n",
      "\n",
      "Wait, but the user might have made a mistake. Let me re-read the problem. The user says: \"What is the main idea of the arxiv paper?\" So they're asking about the main idea of an arXiv paper in general? But arXiv has thousands of papers. The function is supposed to retrieve a specific paper based on a query. So the user's question is too vague. Therefore, the correct response is to ask for the title or ID of the paper. But according to the problem's instructions, the assistant should only output tool calls if they need to. Since the user didn't provide a query, there's no tool call to make. So I shouldn't output any tool_call tags here.\n",
      "\n",
      "But the problem says \"You may call one or more functions to assist with the user query.\" So in this case, since the user's query is incomplete, the assistant should not call the function. Therefore, the answer is no tool calls. But the problem might expect me to generate a tool call with a generic query? Like \"main idea of arxiv paper\" but that's not helpful. Wait, the function's description says \"Retrieves and answers the question about the arxiv paper\". So the query parameter is what to look up. If the user says \"main idea of the arxiv paper\", the query could be \"main idea of the arxiv paper\", but that's not a specific paper. The function might not handle that. So perhaps the best approach is to generate a tool call with the user's query as the query parameter. Wait, but the user's query is \"What is the main idea of the arxiv paper?\" which is a bit unclear. Let me try to see.\n",
      "\n",
      "The function's parameters require a \"query\" string. So maybe the assistant should call the function with the user's question as the query. But the function is supposed to retrieve a specific paper. If the user's query is too vague, the function might return an error. However, the problem says to call the function if possible. So perhaps the assistant should generate a tool call with the user's question as the query. Let me check the function's description: \"Retrieves and answers the question about the arxiv paper\". So the query is what to look up in the retriever. So the user's question is the query. Wait, but the user is asking for the main idea of an arXiv paper, which is a general question. The function might not handle that. But according to the problem's instructions, I need to call the function with the query provided by the user.\n",
      "\n",
      "Wait, the user's message is \"What is the main idea of the arxiv paper?\" So the query parameter would be that string. So the tool call would be arxiv_paper_retriever with query: \"What is the main idea of the arxiv paper?\".\n",
      "\n",
      "But the problem is that the function is supposed to retrieve a specific paper. If the query is too vague, the function might not find a paper. But the problem says to call the function if possible. So maybe the assistant should generate that tool call.\n",
      "\n",
      "Wait, the problem states: \"You may call one or more functions to assist with the user query.\" So the user's query is the input, and the assistant needs to call the function with the query as the parameter. So in this case, the query parameter is the user's question. So the tool call would be:\n",
      "\n",
      "{\n",
      "  \"name\": \"arxiv_paper_retriever\",\n",
      "  \"arguments\": {\n",
      "    \"query\": \"What is the main idea of the arxiv paper?\"\n",
      "  }\n",
      "}\n",
      "\n",
      "But the user's question is a bit ambiguous. However, given the tools provided, this is the only function available, so the assistant should call it with the user's query as the parameter.\n",
      "\n",
      "Wait, but the function's description says \"Retrieves and answers the question about the arxiv paper\". So the query is the question to answer. So the user is asking a question, so the query parameter is the question itself.\n",
      "\n",
      "Yes, so the correct tool call is to use the user's question as the query string.\n",
      "\n",
      "So the tool_call XML would be:\n",
      "Tool Calls:\n",
      "  arxiv_paper_retriever (1536cab5-8ca0-40ba-b21d-b396da296d60)\n",
      " Call ID: 1536cab5-8ca0-40ba-b21d-b396da296d60\n",
      "  Args:\n",
      "    query: What is the main idea of the arxiv paper?\n",
      "  arxiv_paper_retriever (cac54643-6c15-421d-a419-70140c04fc7c)\n",
      " Call ID: cac54643-6c15-421d-a419-70140c04fc7c\n",
      "  Args:\n",
      "    query: What is the main idea of the arxiv paper?\n"
     ]
    }
   ],
   "source": [
    "# Check if the retriever tool is working\n",
    "\n",
    "\n",
    "input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the main idea of the arxiv paper?\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "generate_query_or_respond(input)[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20fbfbb",
   "metadata": {},
   "source": [
    "## Relevance grading\n",
    "We add a lightweight, LLM-based grader that checks if retrieved context is relevant to the user question.\n",
    "\n",
    "Outputs a binary decision that controls the next step:\n",
    "- relevant -> generate answer\n",
    "- not relevant -> rewrite question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91850382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# GRADE_PROMPT â€“ binary relevance grader (returns yes/no)\n",
    "\n",
    "GRADE_PROMPT = (\n",
    "    \"You are an expert RAG evaluator. Decide if the retrieved CONTEXT is \"\n",
    "    \"useful to answer the USER QUESTION.\\n\\n\"\n",
    "    \"Evaluation rubric (relevant = 'yes'):\\n\"\n",
    "    \"- The context contains facts, definitions, steps, or terminology that directly support answering the question.\\n\"\n",
    "    \"- The context matches the semantic intent (not just surface keywords).\\n\"\n",
    "    \"- Minor wording differences are acceptable if the meaning aligns.\\n\"\n",
    "    \"Irrelevant = 'no' when the context is off-topic, purely generic, or lacks the key facts needed.\\n\\n\"\n",
    "    \"USER QUESTION:\\n{question}\\n\\n\"\n",
    "    \"CONTEXT:\\n{context}\\n\\n\"\n",
    "    \"Answer strictly with 'yes' or 'no'.\"\n",
    ")\n",
    "\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"Grade documents using a binary score for relevance check.\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n",
    "    )\n",
    "\n",
    "\n",
    "grader_model = init_chat_model(\"ollama:qwen3:4b\", temperature=0)\n",
    "\n",
    "\n",
    "def grade_documents(\n",
    "    state: MessagesState,\n",
    ") -> Literal[\"generate_answer\", \"rewrite_question\"]:\n",
    "    \"\"\"Determine whether the retrieved documents are relevant to the question.\n",
    "    Input:\n",
    "    - question: str\n",
    "    - context: str\n",
    "    Output:\n",
    "    - binary_score: str\n",
    "\n",
    "    \"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "\n",
    "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
    "    response = grader_model.with_structured_output(GradeDocuments).invoke(\n",
    "        [{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    score = response.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        return \"generate_answer\"\n",
    "    else:\n",
    "        return \"rewrite_question\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69d258b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rewrite_question'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the grader function is working for rewriting the question\n",
    "\n",
    "\n",
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What are the main limitations of the current agent protocols?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"types of reward hacking\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"something else\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e733c649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generate_answer'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the grade documents function is working for generating an answer\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is the main idea of the paper?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"arxiv_paper_retriever\",\n",
    "                        \"args\": {\"query\": \"paper main idea\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"The main idea of the paper is that agent communication is a new and promising research field.\",\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8cfd2c",
   "metadata": {},
   "source": [
    "## Question rewriting\n",
    "If context is not relevant, the agent rewrites the user question to better capture the intended meaning and improve retrieval on the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "839cf122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# REWRITE_PROMPT â€“ intent-preserving question rewrite to improve\n",
    "\n",
    "REWRITE_PROMPT = (\n",
    "    \"Rewrite the user's question to maximize retrieval quality while preserving intent.\\n\"\n",
    "    \"Guidelines:\\n\"\n",
    "    \"- Keep it specific and unambiguous; include essential entities, time ranges, and constraints.\\n\"\n",
    "    \"- Remove vague words; prefer domain terms from the original question when helpful.\\n\"\n",
    "    \"- Keep it a single, self-contained question; do not add new information.\\n\\n\"\n",
    "    \"Original question:\\n-------\\n{question}\\n-------\\n\"\n",
    "    \"Improved question:\"\n",
    ")\n",
    "\n",
    "\n",
    "def rewrite_question(state: MessagesState):\n",
    "    \"\"\"Rewrite the original user question.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    prompt = REWRITE_PROMPT.format(question=question)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": [HumanMessage(content=response.content)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d97e07ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "We are given the original question: \"What is the main idea of the paper?\"\n",
      " The goal is to rewrite it to maximize retrieval quality while preserving intent.\n",
      "\n",
      " Guidelines:\n",
      " - Keep it specific and unambiguous; include essential entities, time ranges, and constraints.\n",
      " - Remove vague words; prefer domain terms from the original question when helpful.\n",
      " - Keep it a single, self-contained question; do not add new information.\n",
      "\n",
      " Analysis of the original question:\n",
      "   - The question is about the \"main idea\" of a \"paper\".\n",
      "   - However, the original question does not specify which paper. This is vague because there are many papers.\n",
      "\n",
      " Problem: Without knowing which paper, the question is too broad and cannot be retrieved effectively.\n",
      "\n",
      " How to improve?\n",
      "   - We must include the essential entity: the paper. But the original question doesn't specify which paper.\n",
      "   - Since the user didn't specify a paper, we have to assume that in the context of the conversation or the system, there might be a specific paper in mind? \n",
      "     However, the instructions say: do not add new information. So we cannot assume a paper.\n",
      "\n",
      " Alternative approach:\n",
      "   - The problem states: \"maximize retrieval quality\". In a typical retrieval system (like a search engine or a database), we need to be specific.\n",
      "\n",
      " But note: the original question is from a user who might be referring to a paper that was previously mentioned? However, the problem says: do not add new information.\n",
      "\n",
      " Since the original question does not specify a paper, we have two options:\n",
      "   Option 1: Assume that the system context already has a paper in mind (like in a conversation) and then the question becomes specific? But the problem says: do not add new information.\n",
      "\n",
      "   Option 2: The improved question must be as specific as possible without adding new information. However, the original question is too vague.\n",
      "\n",
      " Let's think about what the user might have intended. In many contexts, when someone asks \"What is the main idea of the paper?\" they are referring to a specific paper that they have in mind (e.g., the last paper they read, or a paper they are currently working on). But without that context, we cannot know.\n",
      "\n",
      " However, the problem says: \"preserving intent\". The intent is to get the main idea of a paper. But without the paper, we cannot answer.\n",
      "\n",
      " How can we rewrite without adding new information?\n",
      "   - We cannot add the paper title or author because the original question doesn't have it.\n",
      "\n",
      " But note: the guidelines say \"include essential entities\". The essential entity here is the paper. Since the user didn't specify, we have to leave it as a placeholder? \n",
      "\n",
      " However, the problem says: \"do not add new information\". So we cannot invent a paper.\n",
      "\n",
      " Another idea: In some systems, the user might be interacting with a system that has a current context (like a paper that was uploaded or the last paper in the conversation). But the problem doesn't specify that.\n",
      "\n",
      " Given the constraints, the best we can do is to make the question as specific as the original without adding anything. But the original is too vague.\n",
      "\n",
      " Let me re-read the guidelines: \"Keep it specific and unambiguous\". The original is not specific. So we have to make it more specific? But we don't have extra information.\n",
      "\n",
      " Alternative: The problem might be that in the context of the system, there is a paper that is the subject of the conversation. However, the problem says: do not add new information. So we cannot assume.\n",
      "\n",
      " After careful thought, I think the intended solution is to recognize that the original question is too vague and we must force it to be specific by including the fact that it's about the current paper (if the system has a context) but without adding new information, we cannot.\n",
      "\n",
      " However, note the problem says: \"Rewrite the user's question\". The user's question is \"What is the main idea of the paper?\".\n",
      "\n",
      " Since the user didn't specify which paper, the improved question should still be the same? But that doesn't maximize retrieval quality.\n",
      "\n",
      " Let me look for standard practices in information retrieval:\n",
      "\n",
      "   - In a search engine, if you want to get the main idea of a paper, you would need to specify the paper.\n",
      "\n",
      "   - But the problem says: do not add new information.\n",
      "\n",
      " Therefore, the improved question must be the same as the original? But that doesn't seem to maximize retrieval quality.\n",
      "\n",
      " Another angle: The problem might be that the user is in a context where the paper is already known (e.g., the system has a paper object). In that case, the question becomes: \"What is the main idea of the current paper?\".\n",
      "\n",
      " However, the problem says: do not add new information. So we cannot say \"current\" because that's new.\n",
      "\n",
      " How about: We rewrite the question to be more precise by using the term \"paper\" as a domain term? But the original already uses \"paper\".\n",
      "\n",
      " After careful thought, I think the problem expects us to realize that without a specific paper, the question is unanswerable. But the instruction is to rewrite the question to maximize retrieval quality.\n",
      "\n",
      " Since we cannot add a paper title, we have to leave it as is? But the guidelines say to make it specific and unambiguous.\n",
      "\n",
      " Let me try to see if there's a way without adding new information:\n",
      "\n",
      "   Original: \"What is the main idea of the paper?\"\n",
      "\n",
      "   We can rewrite it as: \"What is the main idea of the paper?\" -> but that's the same.\n",
      "\n",
      "   However, the problem says: \"maximize retrieval quality\". In a system that has a paper in context, the system might interpret \"the paper\" as the current paper. But without context, it's vague.\n",
      "\n",
      "   Given the constraints, I think the best we can do is to keep the question as is? But that doesn't seem to follow the guideline of being specific.\n",
      "\n",
      "   Alternatively, the problem might be that the user intended to refer to a specific paper that was previously mentioned in the conversation? But the problem doesn't say that.\n",
      "\n",
      "   Since the problem says: \"do not add new information\", we cannot change the question to include a paper title.\n",
      "\n",
      "   Therefore, the improved question is the same as the original? But that doesn't maximize retrieval quality.\n",
      "\n",
      "   However, note: the problem says \"Rewrite the user's question\". We are allowed to rephrase to be more specific without adding new information? But the original is already specific? \n",
      "\n",
      "   Wait, the original is not specific because it doesn't say which paper.\n",
      "\n",
      "   But the problem says: \"include essential entities\". The essential entity is the paper. We don't have it.\n",
      "\n",
      "   I think the intended answer here is to realize that without a specific paper, the question is unanswerable. But the problem asks for a rewrite that preserves intent.\n",
      "\n",
      "   After reading the problem again: \"maximize retrieval quality\". In a retrieval system, if we don't specify the paper, the system might return multiple papers. So we want to avoid that.\n",
      "\n",
      "   How about: We rewrite the question to be more precise by using the term \"the paper\" (implying the current context) without adding new information? But the problem says: do not add new information.\n",
      "\n",
      "   I found a solution in the literature: In some systems, when the user says \"the paper\", it refers to the paper that was just discussed. But without context, we can't.\n",
      "\n",
      "   Given the strict constraints, I think the problem expects us to output the same question? But that doesn't seem to be maximizing retrieval quality.\n",
      "\n",
      "   Let me try to think of what the problem might be testing.\n",
      "\n",
      "   Another idea: The problem says \"preserving intent\". The intent is to get the main idea of a paper. We can rephrase to be more precise by using the term \"paper\" as a domain term? But it's already a domain term.\n",
      "\n",
      "   I think the answer is: We cannot make it more specific without adding information. So we leave it as is? But the problem says \"maximize retrieval quality\", so we want to be as specific as possible.\n",
      "\n",
      "   However, the guidelines say: \"Remove vague words\". The word \"the\" is vague? Actually, the vagueness is the lack of a specific paper.\n",
      "\n",
      "   Since we cannot add a paper title, we have to accept that the question is vague. But the problem says: \"maximize retrieval quality\". So we might have to make it as specific as we can without adding information.\n",
      "\n",
      "   I propose: The improved question is: \"What is the main idea of the paper?\" -> but that's the same.\n",
      "\n",
      "   Wait, the problem says: \"Rewrite\". So we can rephrase the sentence to be more precise? But without adding information, we can't.\n",
      "\n",
      "   After careful thought, I think the problem might be that in the context of the system, there is a paper that is the subject of the conversation. So the improved question is: \"What is the main idea of the paper?\" (same) but we can add a constraint that it's the current paper? But we cannot add new information.\n",
      "\n",
      "   Given the above, I decide to output the same question because we cannot add new information and we don't have the paper title.\n",
      "\n",
      "   However, the problem says: \"maximize retrieval quality\". In practice, if the system has a paper in context, then the question is specific enough. So we don't change it.\n",
      "\n",
      "   But note: the problem says \"include essential entities\". The essential entity is the paper. We don't have it, so we leave it as \"the paper\" (which is a common way to refer to the current paper in context).\n",
      "\n",
      "   Therefore, the improved question is: \"What is the main idea of the paper?\"\n",
      "\n",
      "   However, the problem says: \"Remove vague words\". The word \"the\" might be considered vague? But in English, \"the paper\" in context usually refers to one specific paper.\n",
      "\n",
      "   Alternatively, we can write: \"What is the main idea of the paper?\" -> but that's the same.\n",
      "\n",
      "   I think the answer is the same.\n",
      "\n",
      "   But wait, the problem says: \"maximize retrieval quality\". If we rewrite it to be more specific by using the term \"the paper\" (which is a common term in academic contexts to refer to the paper being discussed), then it's already specific enough.\n",
      "\n",
      "   So I'll go with: \"What is the main idea of the paper?\"\n",
      "\n",
      "   However, the problem says: \"Keep it specific and unambiguous\". The original is ambiguous because it doesn't specify which paper. But in the context of the conversation, it's unambiguous.\n",
      "\n",
      "   Given the constraints, I think the improved question is the same.\n",
      "\n",
      "   But let me see if there's a standard way.\n",
      "\n",
      "   Another thought: In some systems, the user might have uploaded a paper. Then the question is about that paper. So the question is specific.\n",
      "\n",
      "   Therefore, I conclude: The improved question is the same as the original.\n",
      "\n",
      "   However, the problem says: \"Rewrite\". So we are allowed to rephrase? But without adding information, we can't change the meaning.\n",
      "\n",
      "   I think the answer is: \"What is the main idea of the paper?\"\n",
      "\n",
      "   But wait, the problem says: \"maximize retrieval quality\". If we rewrite it to be more precise by using the term \"the paper\" (which is a standard term in academic contexts for the paper being discussed), then it's already good.\n",
      "\n",
      "   So I'll output: \"What is the main idea of the paper?\"\n",
      "\n",
      "   However, the problem might expect a different answer because the original is too vague.\n",
      "\n",
      "   Let me check the guidelines again: \"Keep it specific and unambiguous\". The original is not specific. So we have to make it specific? But we don't have the paper title.\n",
      "\n",
      "   I think the problem is designed to have us recognize that without a specific paper, we cannot make it specific. So we leave it as is.\n",
      "\n",
      "   Final decision: The improved question is the same as the original.\n",
      "\n",
      "   But note: the problem says \"Rewrite\". So we are not changing it? That seems odd.\n",
      "\n",
      "   Alternative: In the context of the system, the paper might be identified by a unique identifier. But the user didn't provide it.\n",
      "\n",
      "   Given the instructions, I think the answer is: \"What is the main idea of the paper?\"\n",
      "\n",
      "   However, I recall that in some retrieval systems, they use the term \"the paper\" to mean the paper that was just discussed. So it's acceptable.\n",
      "\n",
      "   Let's write it as: \"What is the main idea of the paper?\"\n",
      "\n",
      "   But the problem says: \"do not add new information\". We are not adding anything.\n",
      "\n",
      "   So I'll go with that.\n",
      "\n",
      "   However, the problem says: \"maximize retrieval quality\". If the system has a paper in context, then this question will be retrieved as the current paper. If not, then it's vague. But we have to work with what we have.\n",
      "\n",
      "   Therefore, the improved question is: \"What is the main idea of the paper?\"\n",
      "\n",
      "   But wait, the problem says: \"include essential entities\". We have the entity \"paper\", but we don't have a specific one. So it's not specific.\n",
      "\n",
      "   I think the problem might be expecting us to realize that we need to specify the paper. But we can't.\n",
      "\n",
      "   After re-thinking, I found a solution: In the context of the conversation, the user might have referred to a paper in a previous message. But the problem doesn't say that.\n",
      "\n",
      "   Given the strict constraints, I output the same.\n",
      "\n",
      "   However, the problem says: \"Rewrite\". So we can rephrase the sentence to be more precise? For example, we can say: \"What is the main idea of the paper?\" -> but that's the same.\n",
      "\n",
      "   I think the answer is: \"What is the main idea of the paper?\"\n",
      "\n",
      "   Let me see if there's a way to make it more specific without adding information? \n",
      "\n",
      "   We can use the term \"this paper\" to imply the current context? But the problem says: do not add new information. The word \"this\" is new? \n",
      "\n",
      "   The original says \"the paper\", so we can change to \"this paper\" to be more specific? But that adds the word \"this\", which is a new word? \n",
      "\n",
      "   However, in the context of the conversation, \"this paper\" refers to the paper that was just discussed. And it's a common way to be specific.\n",
      "\n",
      "   But the problem says: \"do not add new information\". The word \"this\" is not new information? It's a pronoun that refers to the current context.\n",
      "\n",
      "   Let me check: The original question uses \"the paper\". We are changing to \"this paper\". Is that adding new information? \n",
      "\n",
      "   In English, \"this\" is a demonstrative pronoun that is used to refer to something specific in the current context. It's not adding new information because it's part of the language.\n",
      "\n",
      "   However, the problem says: \"do not add new information\". The information that \"this\" implies is that the paper is the one being discussed now. But the original question didn't have that context. \n",
      "\n",
      "   But the problem says: \"preserving intent\". The intent is to get the main idea of the paper that the user is currently talking about. So it's the same intent.\n",
      "\n",
      "   And the guidelines say: \"Remove vague words\". The word \"the\" might be vague because it doesn't specify which paper, but \"this\" is more specific.\n",
      "\n",
      "   So I think: \"What is the main idea of this paper?\"\n",
      "\n",
      "   Why is this better?\n",
      "     - It's more specific: it refers to the paper that is currently in the conversation (the one the user is talking about).\n",
      "     - It doesn't add new information: we are just using a pronoun that is standard in conversational English to refer to the current context.\n",
      "\n",
      "   But note: the problem says \"do not add new information\". The word \"this\" is not new information? It's a word that has been used in English for a long time. And it doesn't add a new fact about the paper.\n",
      "\n",
      "   Also, the problem says: \"prefer domain terms from the original question\". The original question has \"paper\", and \"this\" is a common term in the context of academic discussions.\n",
      "\n",
      "   Let me see if this is acceptable.\n",
      "\n",
      "   In academic writing, we often say \"this paper\" to refer to the paper being discussed.\n",
      "\n",
      "   Example: In a paper, you might say: \"This paper presents ...\".\n",
      "\n",
      "   So I think this rewrite is better.\n",
      "\n",
      "   Steps:\n",
      "     - Original: \"What is the main idea of the paper?\"\n",
      "     - Improved: \"What is the main idea of this paper?\"\n",
      "\n",
      "   Why is it more specific and unambiguous?\n",
      "     - \"this paper\" is unambiguous in the context of the conversation (it refers to the paper that was just mentioned or the current topic).\n",
      "     - It removes the vagueness of \"the paper\" (which could be any paper).\n",
      "\n",
      "   And we are not adding new information: we are just changing \"the\" to \"this\", which is a standard way to be specific in conversation.\n",
      "\n",
      "   Also, it's a single, self-contained question.\n",
      "\n",
      "   Therefore, I choose: \"What is the main idea of this paper?\"\n",
      "\n",
      "   Let me check the guidelines:\n",
      "     - Specific and unambiguous: Yes, because it refers to the current paper in context.\n",
      "     - Remove vague words: The word \"the\" is vague (it doesn't specify which paper), so we remove it by using \"this\" (which is more specific).\n",
      "     - Prefer domain terms: In academic contexts, \"this paper\" is a common term.\n",
      "\n",
      "   This seems to maximize retrieval quality because the system will now look for the main idea of the paper that is the subject of the current conversation.\n",
      "\n",
      "   So the improved question is: \"What is the main idea of this paper?\"\n",
      "\n",
      "   But note: the problem says \"do not add new information\". We are not adding new information about the paper, just a pronoun that is standard.\n",
      "\n",
      "   I think this is the answer.\n",
      "\n",
      "   However, what if the system doesn't have a current paper? Then it's still vague. But the problem says: preserve intent. The intent is to get the main idea of the paper that the user is currently discussing.\n",
      "\n",
      "   Given the above, I'll go with: \"What is the main idea of this paper?\"\n",
      "\n",
      "   Let's write it.\n",
      "\n",
      "   Final answer: \"What is the main idea of this paper?\"\n",
      "</think>\n",
      "\n",
      "What is the main idea of this paper?\n"
     ]
    }
   ],
   "source": [
    "# Check if the rewrite question function is working\n",
    "\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is the main idea of the paper?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"arxiv_paper_retriever\",\n",
    "                        \"args\": {\"query\": \"paper main idea\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = rewrite_question(input)\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c422fd37",
   "metadata": {},
   "source": [
    "## Answer generation\n",
    "Given a user question and relevant context, the model produces a concise answer.\n",
    "\n",
    "Principles:\n",
    "- be faithful to provided context\n",
    "- be brief and clear; admit when unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# GENERATE_PROMPT â€“ grounded answer generation from provided context\n",
    "\n",
    "GENERATE_PROMPT = (\n",
    "    \"You are a concise, trustworthy assistant for Retrieval-Augmented QA.\\n\"\n",
    "    \"Instructions:\\n\"\n",
    "    \"- Use only the information in CONTEXT. Do not invent facts.\\n\"\n",
    "    '- If the answer is not found, say: \"I don\\'t know based on the provided context.\"\\n'\n",
    "    \"- Prefer 2â€“4 clear sentences. Use bullet points only if enumerating items.\\n\"\n",
    "    \"- Be faithful to the source; avoid speculation.\"\n",
    ")\n",
    "\n",
    "\n",
    "def generate_answer(state: MessagesState):\n",
    "    \"Generate an answer for a given question with the provided context.\"\n",
    "\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        elif message.type in (\"human\", \"ai\"):\n",
    "            break\n",
    "    tool_messages = list(reversed(recent_tool_messages))\n",
    "\n",
    "    # Limit the number of tool messages\n",
    "    MAX_TOOL_MSG = 3\n",
    "    tool_messages = tool_messages[-MAX_TOOL_MSG:]\n",
    "\n",
    "    # Get the context from the tool messages\n",
    "    context = \"\\n\\n\".join(doc.content for doc in tool_messages) if tool_messages else \"\"\n",
    "\n",
    "    # Get the question from the state\n",
    "    question = None\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"human\":\n",
    "            question = message.content\n",
    "            break\n",
    "    if not question:\n",
    "        raise ValueError(\"No question found in the state\")\n",
    "\n",
    "    # Format the prompt\n",
    "    system = SystemMessage(content=GENERATE_PROMPT)\n",
    "\n",
    "    if context.strip():\n",
    "        user_text = f\"QUESTION:\\n{question}\\n\\nCONTEXT:\\n{context}\\n\\nFinal answer:\"\n",
    "    else:\n",
    "        # handle empty context\n",
    "        user_text = (\n",
    "            f\"QUESTION:\\n{question}\\n\\n(No external CONTEXT was retrieved.)\\n\\nFinal answer:\"\n",
    "        )\n",
    "\n",
    "    user = HumanMessage(content=user_text)\n",
    "    response = response_model.invoke([system, user])\n",
    "\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d099069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Hmm, the user is asking about the main idea of a paper based on the provided context. I need to be careful here - they want a concise, trustworthy response that only uses the context given.\n",
      "\n",
      "Looking at the context, it describes a paper that defines agent communication and categorizes its lifecycle into three stages: user-agent interaction, agent-agent communication, and agent-environment communication. The paper then analyzes security risks for each phase, summarizes defense countermeasures, conducts experiments with MCP and A2A, and discusses future directions.\n",
      "\n",
      "The key here is to extract the core purpose without adding anything. The context clearly states this is about \"a clear definition of agent communication\" and the subsequent analysis of security risks across communication phases. The experiments and future directions are supporting elements rather than the main idea itself.\n",
      "\n",
      "I should avoid inventing facts - the context doesn't mention what the paper is about beyond this description. The main idea seems to be the framework for understanding and securing agent communication systems through this three-stage lifecycle approach.\n",
      "\n",
      "I'll keep it to 2-4 clear sentences as requested. No bullet points since it's not enumerating items. Must say \"I don't know\" only if context is missing, but here we have enough information.\n",
      "\n",
      "The response should focus on: 1) defining agent communication, 2) categorizing lifecycle into three stages, 3) analyzing security risks per stage, 4) with experiments and future work as supporting elements. But I'll condense this to the essence without getting too detailed.\n",
      "\n",
      "Important to note the paper is about security risks in agent communication systems - that's the central theme according to the context. The experiments with MCP and A2A are just tools to demonstrate vulnerabilities, not the main idea.\n",
      "</think>\n",
      "\n",
      "The main idea of the paper is to define agent communication and categorize its lifecycle into three distinct stages: user-agent interaction, agent-agent communication, and agent-environment communication. It then analyzes security risks for each stage and proposes defense countermeasures, supported by experiments using MCP and A2A to demonstrate vulnerabilities. The paper concludes with open issues and future research directions in this security-focused area.\n"
     ]
    }
   ],
   "source": [
    "# Check if the answer generation function is working\n",
    "\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is the main idea of the paper?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"arxiv_paper_retriever\",\n",
    "                        \"args\": {\"query\": \"paper main idea\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"More precisely, we first present a clear definition of agent communication and categorize the entire lifecycle of agent communication into three stages: user-agent interaction, agent-agent communication, and agent-environment communication. Next, for each communication phase, we dissect related protocols and analyze the security risks according to the communication characteristics. Then, we summarize and outlook on the possible defense countermeasures for each risk. In addition, we conduct experiments using MCP and A2A to help readers better understand the novel vulnerabilities brought by agent communication. Finally, we discuss open issues and future directions in this promising research field.\",\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = generate_answer(input)\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8d00ae",
   "metadata": {},
   "source": [
    "## Agent workflow (LangGraph)\n",
    "We compose the functions into a small state graph. The flow is:\n",
    "1. decide (respond or use tool)\n",
    "2. if tool used -> grade relevance\n",
    "3. if relevant -> generate answer; else -> rewrite question and loop\n",
    "\n",
    "The graph makes the agent's control flow explicit and inspectable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f35a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(generate_query_or_respond)\n",
    "workflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\n",
    "workflow.add_node(rewrite_question)\n",
    "workflow.add_node(generate_answer)\n",
    "\n",
    "workflow.add_edge(START, \"generate_query_or_respond\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_query_or_respond\",\n",
    "    # Assess LLM decision (call `retriever_tool` tool or respond to the user)\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "workflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c1d9f9",
   "metadata": {},
   "source": [
    "## Visualize the workflow\n",
    "We render the compiled state graph to verify the structure and for easy explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d1d3955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a935c131",
   "metadata": {},
   "source": [
    "## Streaming the run\n",
    "We stream node updates to observe the agent's decisions step-by-step. This helps debug and explain the agentic behavior in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d9c4fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the main contribution of the 'A Survey of LLM-Driven AI Agent Communication' paper?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, the user is asking about the main contribution of the paper titled 'A Survey of LLM-Driven AI Agent Communication'. I need to figure out how to get that information. The tools provided include an arxiv_paper_retriever function that takes a query. So I should use that function to look up the paper.\n",
      "\n",
      "First, I'll check if the title is correctly formatted. The user wrote 'A Survey of LLM-Driven AI Agent Communication'. Maybe the exact title on arXiv has some variations, but the function should handle the query. I'll structure the query as the title given.\n",
      "\n",
      "The function requires a \"query\" parameter, which is a string. So I'll set the query to the title the user mentioned. Let me make sure to use the exact title they provided. Sometimes titles have different capitalizations or punctuation, but since the user specified it that way, I'll go with it.\n",
      "\n",
      "So the tool call should be arxiv_paper_retrieas with the query being \"A Survey of LLM-Driven AI Agent Communication\". Wait, the function name is arxiv_paper_retriever, but in the tool definition it's written as \"arxiv_paper_retriever\". Let me check the tool definition again.\n",
      "\n",
      "Yes, the tool is named \"arxiv_paper_retriever\". So the function name is correct. The parameters require a \"query\" string. So I'll generate the tool call with that query.\n",
      "\n",
      "I think that's all. The user wants the main contribution, so the function should retrieve the paper's information and answer that. Since I can't actually access the arXiv database, the tool call is just to initiate the retrieval process. So the response should be a tool call with the specified query.\n",
      "</think>\n",
      "Tool Calls:\n",
      "  arxiv_paper_retriever (ca3a61ea-831b-4429-bc33-1d84c1361804)\n",
      " Call ID: ca3a61ea-831b-4429-bc33-1d84c1361804\n",
      "  Args:\n",
      "    query: A Survey of LLM-Driven AI Agent Communication\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_paper_retriever\n",
      "\n",
      "A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures\n",
      "Dezhang Kong, Shi Lin, Zhenhua Xu, Zhebo Wang, Minghao Li, Yufeng Li, Yilun Zhang, Hujin Peng, Zeyang Sha, Yuyuan Li, Changting Lin, Xun Wang, Xuan Liu, Ningyu Zhang, Chaochao Chen, Muhammad Khurram Khan, Meng Han\n",
      "\n",
      "A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures\n",
      "Index Terms -large language model, agent communication, attack, and security\n",
      "\n",
      "A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures\n",
      "Abstract -In recent years, Large-Language-Model-driven AI agents have exhibited unprecedented intelligence and adaptability, and are rapidly changing human production and life. Nowadays, agents are undergoing a new round of evolution. They no longer act as an isolated island like LLMs. Instead, they start to communicate with diverse external entities, such as other agents and tools, to perform more complex tasks collectively. Under this trend, agent communication is regarded as a foundational pillar of the future AI ecosystem, and many organizations have intensively begun\n",
      "\n",
      "A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures\n",
      "to design related communication protocols (e.g., Anthropic's MCP and Google's A2A) within the recent few months. However, this new field exposes significant security hazards, which can cause severe damage to real-world scenarios. To help researchers quickly figure out this promising topic and benefit the future agent communication development, this paper presents a comprehensive survey of agent communication security . More precisely, we first present a clear definition of agent communication and categorize the entire lifecycle of agent communication into\n",
      "\n",
      "A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures\n",
      "three stages: user-agent interaction, agent-agent communication, and agent-environment communication. Next, for each communication phase, we dissect related protocols and analyze the security risks according to the communication characteristics. Then, we summarize and outlook on the possible defense countermeasures for each risk. In addition, we conduct experiments using MCP and A2A to help readers better understand the novel vulnerabilities brought by agent communication. Finally, we discuss open issues and future directions in this promising research field.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Hmm, the user is asking about the main contribution of a specific AI paper titled \"A Survey of LLM-Driven AI Agent Communication.\" They want a concise answer based strictly on the provided context. \n",
      "\n",
      "Let me scan the context carefully. The abstract section gives the clearest answer - it states the paper presents \"a comprehensive survey of agent communication security\" with several key components. The paper defines agent communication, categorizes its lifecycle into three stages (user-agent, agent-agent, agent-environment), analyzes security risks for each phase, summarizes defense countermeasures, and conducts experiments with real protocols like MCP and A2A.\n",
      "\n",
      "I notice the user emphasized strict adherence to context, so I must avoid adding any external knowledge. The paper's contribution isn't just about protocols but specifically the security analysis framework across communication stages. The experiments with MCP/A2A are part of the methodology but not the core contribution.\n",
      "\n",
      "The answer should be tight - 2-4 sentences max. I'll focus on: 1) The comprehensive security survey framework 2) The three-stage lifecycle categorization 3) Risk analysis per stage 4) Defense countermeasures. No need to mention the experiments since they're secondary to the core contribution.\n",
      "\n",
      "Also checking for traps: The title mentions \"LLM-Driven\" but the context shows this is about agent communication in general (with LLMs as part of the ecosystem). Must not overstate - the paper's contribution is specifically the security survey framework, not LLMs themselves.\n",
      "\n",
      "User seems technical (using precise terms like \"retrieval-augmented qa\"), so I'll keep the language professional but simple. They'll appreciate directness since they want conciseness.\n",
      "</think>\n",
      "\n",
      "Based solely on the provided context, the main contribution of the paper is a comprehensive security survey framework for LLM-driven AI agent communication. It defines agent communication, categorizes the entire lifecycle into three distinct stages (user-agent interaction, agent-agent communication, and agent-environment communication), and analyzes security risks for each stage while summarizing corresponding defense countermeasures. This work provides a structured approach to understanding vulnerabilities and mitigation strategies in emerging agent communication protocols.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"thread_1\"}}\n",
    "\n",
    "input_message = (\n",
    "    \"What is the main contribution of the 'A Survey of LLM-Driven AI Agent Communication' paper?\"\n",
    ")\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": input_message,\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa9a397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Who's the main author?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, let me try to figure out who the main author is for this paper. The user asked, \"Who's the main author?\" after I provided the context about the paper.\n",
      "\n",
      "First, I need to check the information given in the tool response. The tool response shows the paper title and the list of authors. Let me look at that.\n",
      "\n",
      "The authors listed are: Dezhang Kong, Shi Lin, Zhenhua Xu, Zhebo Wang, Minghao Li, Yufeng Li, Yilun Zhang, Hujin Peng, Zeyang Sha, Yuyuan Li, Changting Lin, Xun Wang, Xuan Liu, Ningyu Zhang, Chaochao Chen, Muhammad Khurram Khan, Meng Han.\n",
      "\n",
      "In academic papers, the main author is usually the first author listed. So in this case, the first name is Dezhang Kong. But sometimes, the order might vary depending on the institution or the country, but generally, the first author is considered the main contributor.\n",
      "\n",
      "Wait, the tool response shows the authors in a list. Let me check the exact order. The first author here is Dezhang Kong. So the main author should be Dezhang Kong.\n",
      "\n",
      "I should make sure there's no other information that might indicate a different main author. The abstract and the index terms don't mention any specific lead author beyond the list. The paper's title is \"A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures\", and the authors are listed in the order provided.\n",
      "\n",
      "So the answer is Dezhang Kong.\n",
      "</think>\n",
      "\n",
      "The main author of the paper \"A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures\" is **Dezhang Kong**, as they are listed first in the author hierarchy.\n"
     ]
    }
   ],
   "source": [
    "input_message = \"Who's the main author?\"\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": input_message,\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f458934",
   "metadata": {},
   "source": [
    "## Inspecting the chat history\n",
    "\n",
    "We can check the stored conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3a60ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the main contribution of the 'A Survey of LLM-Driven AI Agent Communication' paper?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, the user is asking about the main contribution of the paper titled 'A Survey of LLM-Driven AI Agent Communication'. I need to figure out how to get that information. The tools provided include an arxiv_paper_retriever function that takes a query. So I should use that function to look up the paper.\n",
      "\n",
      "First, I'll check if the title is correctly formatted. The user wrote 'A Survey of LLM-Driven AI Agent Communication'. Maybe the exact title on arXiv has some variations, but the function should handle the query. I'll structure the query as the title given.\n",
      "\n",
      "The function requires a \"query\" parameter, which is a string. So I'll set the query to the title the user mentioned. Let me make sure to use the exact title they provided. Sometimes titles have different capitalizations or punctuation, but since the user specified it that way, I'll go with it.\n",
      "\n",
      "So the tool call should be arxiv_paper_retrieas with the query being \"A Survey of LLM-Driven AI Agent Communication\". Wait, the function name is arxiv_paper_retriever, but in the tool definition it's written as \"arxiv_paper_retriever\". Let me check the tool definition again.\n",
      "\n",
      "Yes, the tool is named \"arxiv_paper_retriever\". So the function name is correct. The parameters require a \"query\" string. So I'll generate the tool call with that query.\n",
      "\n",
      "I think that's all. The user wants the main contribution, so the function should retrieve the paper's information and answer that. Since I can't actually access the arXiv database, the tool call is just to initiate the retrieval process. So the response should be a tool call with the specified query.\n",
      "</think>\n",
      "Tool Calls:\n",
      "  arxiv_paper_retriever (ca3a61ea-831b-4429-bc33-1d84c1361804)\n",
      " Call ID: ca3a61ea-831b-4429-bc33-1d84c1361804\n",
      "  Args:\n",
      "    query: A Survey of LLM-Driven AI Agent Communication\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_paper_retriever\n",
      "\n",
      "A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures\n",
      "Dezhang Kong, Shi Lin, Zhenhua Xu, Zhebo Wang, Minghao Li, Yufeng Li, Yilun Zhang, Hujin Peng, Zeyang Sha, Yuyuan Li, Changting Lin, Xun Wang, Xuan Liu, Ningyu Zhang, Chaochao Chen, Muhammad Khurram Khan, Meng Han\n",
      "\n",
      "A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures\n",
      "Index Terms -large language model, agent communication, attack, and security\n",
      "\n",
      "A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures\n",
      "Abstract -In recent years, Large-Language-Model-driven AI agents have exhibited unprecedented intelligence and adaptability, and are rapidly changing human production and life. Nowadays, agents are undergoing a new round of evolution. They no longer act as an isolated island like LLMs. Instead, they start to communicate with diverse external entities, such as other agents and tools, to perform more complex tasks collectively. Under this trend, agent communication is regarded as a foundational pillar of the future AI ecosystem, and many organizations have intensively begun\n",
      "\n",
      "A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures\n",
      "to design related communication protocols (e.g., Anthropic's MCP and Google's A2A) within the recent few months. However, this new field exposes significant security hazards, which can cause severe damage to real-world scenarios. To help researchers quickly figure out this promising topic and benefit the future agent communication development, this paper presents a comprehensive survey of agent communication security . More precisely, we first present a clear definition of agent communication and categorize the entire lifecycle of agent communication into\n",
      "\n",
      "A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures\n",
      "three stages: user-agent interaction, agent-agent communication, and agent-environment communication. Next, for each communication phase, we dissect related protocols and analyze the security risks according to the communication characteristics. Then, we summarize and outlook on the possible defense countermeasures for each risk. In addition, we conduct experiments using MCP and A2A to help readers better understand the novel vulnerabilities brought by agent communication. Finally, we discuss open issues and future directions in this promising research field.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Hmm, the user is asking about the main contribution of a specific AI paper titled \"A Survey of LLM-Driven AI Agent Communication.\" They want a concise answer based strictly on the provided context. \n",
      "\n",
      "Let me scan the context carefully. The abstract section gives the clearest answer - it states the paper presents \"a comprehensive survey of agent communication security\" with several key components. The paper defines agent communication, categorizes its lifecycle into three stages (user-agent, agent-agent, agent-environment), analyzes security risks for each phase, summarizes defense countermeasures, and conducts experiments with real protocols like MCP and A2A.\n",
      "\n",
      "I notice the user emphasized strict adherence to context, so I must avoid adding any external knowledge. The paper's contribution isn't just about protocols but specifically the security analysis framework across communication stages. The experiments with MCP/A2A are part of the methodology but not the core contribution.\n",
      "\n",
      "The answer should be tight - 2-4 sentences max. I'll focus on: 1) The comprehensive security survey framework 2) The three-stage lifecycle categorization 3) Risk analysis per stage 4) Defense countermeasures. No need to mention the experiments since they're secondary to the core contribution.\n",
      "\n",
      "Also checking for traps: The title mentions \"LLM-Driven\" but the context shows this is about agent communication in general (with LLMs as part of the ecosystem). Must not overstate - the paper's contribution is specifically the security survey framework, not LLMs themselves.\n",
      "\n",
      "User seems technical (using precise terms like \"retrieval-augmented qa\"), so I'll keep the language professional but simple. They'll appreciate directness since they want conciseness.\n",
      "</think>\n",
      "\n",
      "Based solely on the provided context, the main contribution of the paper is a comprehensive security survey framework for LLM-driven AI agent communication. It defines agent communication, categorizes the entire lifecycle into three distinct stages (user-agent interaction, agent-agent communication, and agent-environment communication), and analyzes security risks for each stage while summarizing corresponding defense countermeasures. This work provides a structured approach to understanding vulnerabilities and mitigation strategies in emerging agent communication protocols.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Who's the main author?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, let me try to figure out who the main author is for this paper. The user asked, \"Who's the main author?\" after I provided the context about the paper.\n",
      "\n",
      "First, I need to check the information given in the tool response. The tool response shows the paper title and the list of authors. Let me look at that.\n",
      "\n",
      "The authors listed are: Dezhang Kong, Shi Lin, Zhenhua Xu, Zhebo Wang, Minghao Li, Yufeng Li, Yilun Zhang, Hujin Peng, Zeyang Sha, Yuyuan Li, Changting Lin, Xun Wang, Xuan Liu, Ningyu Zhang, Chaochao Chen, Muhammad Khurram Khan, Meng Han.\n",
      "\n",
      "In academic papers, the main author is usually the first author listed. So in this case, the first name is Dezhang Kong. But sometimes, the order might vary depending on the institution or the country, but generally, the first author is considered the main contributor.\n",
      "\n",
      "Wait, the tool response shows the authors in a list. Let me check the exact order. The first author here is Dezhang Kong. So the main author should be Dezhang Kong.\n",
      "\n",
      "I should make sure there's no other information that might indicate a different main author. The abstract and the index terms don't mention any specific lead author beyond the list. The paper's title is \"A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures\", and the authors are listed in the order provided.\n",
      "\n",
      "So the answer is Dezhang Kong.\n",
      "</think>\n",
      "\n",
      "The main author of the paper \"A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures\" is **Dezhang Kong**, as they are listed first in the author hierarchy.\n"
     ]
    }
   ],
   "source": [
    "chat_history = graph.get_state(config).values[\"messages\"]\n",
    "for message in chat_history:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851d3036",
   "metadata": {},
   "source": [
    "# Basic offline evaluation with NDCG@k\n",
    "\n",
    "We check if the chunks' ranking is sufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61059385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gh/wtmb1km14f710f4f7s07wk380000gn/T/ipykernel_32598/879247267.py:45: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@5 report: {'k': 5, 'num_queries': 5, 'ndcg@k_mean': 0.35385596211497444, 'per_query': [0.0, 0.0, 0.0, 1.0, 0.7692798105748722]}\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "from typing import List\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "\n",
    "def dcg(relevances: List[int]) -> float:\n",
    "    \"\"\"\n",
    "    Discounted Cumulative Gain for a ranked list of binary relevances.\n",
    "    rel in {0,1}; rank starts from 1; DCG = sum_{i}(rel_i / log2(i+1)).\n",
    "    \"\"\"\n",
    "    return sum((rel / log2(idx + 2)) for idx, rel in enumerate(relevances))\n",
    "\n",
    "\n",
    "def ndcg_at_k(relevances: List[int], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Compute NDCG@k for a ranked list of binary relevances.\n",
    "    - relevances: list of 0/1 labels in rank order (retriever output order)\n",
    "    - k: cutoff\n",
    "    \"\"\"\n",
    "    r = relevances[:k]\n",
    "    ideal = sorted(r, reverse=True)\n",
    "    return ndcg_score([r], [ideal])\n",
    "\n",
    "\n",
    "def grade_binary(question: str, context: str) -> int:\n",
    "    \"\"\"\n",
    "    Binary relevance using the existing grader LLM (yes->1, no->0).\n",
    "    Reuses GRADE_PROMPT + GradeDocuments for consistency.\n",
    "    \"\"\"\n",
    "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
    "    resp = grader_model.with_structured_output(GradeDocuments).invoke(\n",
    "        [{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return 1 if resp.binary_score == \"yes\" else 0\n",
    "\n",
    "\n",
    "def evaluate_ndcg(queries: List[str], k: int = 5) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate NDCG@k over a set of queries using the current retriever and LLM-based binary labels.\n",
    "    Returns a dict with mean NDCG and per-query breakdown.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for q in queries:\n",
    "        docs = retriever.get_relevant_documents(q)\n",
    "        relevances = [grade_binary(q, d.page_content) for d in docs]\n",
    "        scores.append(ndcg_at_k(relevances, k))\n",
    "    return {\n",
    "        \"k\": k,\n",
    "        \"num_queries\": len(queries),\n",
    "        \"ndcg@k_mean\": float(np.mean(scores)) if scores else 0.0,\n",
    "        \"per_query\": scores,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example local evaluation set (kept small for fast local run)\n",
    "queries = [\n",
    "    \"What was the challange in the selection Principles of the Most Relevant Surveys?\",\n",
    "    \"What stages of agent communication are discussed?\",\n",
    "    \"How many types of relevant surveys are there?\",\n",
    "    \"Which figure illustrates a typical architecture of LLM-driven agents?\",\n",
    "    \"Do agents have worse security than LLMs?\",\n",
    "]\n",
    "ndcg_report = evaluate_ndcg(queries, k=5)\n",
    "print(\"NDCG@5 report:\", ndcg_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
